{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"chores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v1')\n",
    "embeddings = model.encode(df['Chore'].values, convert_to_numpy = True)\n",
    "embeddings_kw = model.encode(df['ChoreKeyword'].values, convert_to_numpy = True)\n",
    "np.save(\"embeddings.npy\", embeddings)\n",
    "np.save(\"embeddings_kw.npy\", embeddings_kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difficulty Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (norm(a) * norm(b))\n",
    "\n",
    "difficult_examples = [\n",
    "    \"Lavoro noioso\",\n",
    "    \"Lavoro complicato\",\n",
    "    \"Lavoro disgustoso\",\n",
    "    \"Lavoro che impiega molto tempo\",\n",
    "]\n",
    "difficult_embeddings = model.encode(difficult_examples, convert_to_numpy=True)\n",
    "\n",
    "for i, label in enumerate(difficult_examples):\n",
    "    df[label] = [cosine_similarity(emb, difficult_embeddings[i]) for emb in embeddings_kw]\n",
    "\n",
    "df.rename(columns={\n",
    "    \"Lavoro noioso\": \"Noioso\",\n",
    "    \"Lavoro complicato\": \"Complicato\",\n",
    "    \"Lavoro disgustoso\": \"Disgustoso\",\n",
    "    \"Lavoro che impiega molto tempo\": \"Tempo\"\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChoreKeyword: Auto - True Difficulty: 2.5 - Predicted: 1.83\n",
      "(Lavatrice (Diff: 1.5, Sim: 0.5439); Lavatrice (filtro) (Diff: 2.0, Sim: 0.5347); Lavandino (Diff: 2.0, Sim: 0.5303))\n",
      "---\n",
      "ChoreKeyword: Indifferenziato - True Difficulty: 1.0 - Predicted: 1.67\n",
      "(Plastica (Diff: 1.0, Sim: 0.4617); Dispensa (Diff: 1.5, Sim: 0.4522); Bidoni differenziata (Diff: 2.5, Sim: 0.4202))\n",
      "---\n",
      "ChoreKeyword: Armadio - True Difficulty: 2.0 - Predicted: 1.83\n",
      "(Dispensa (Diff: 1.5, Sim: 0.6059); Bidet (Diff: 2.0, Sim: 0.5801); Spazzatura (Diff: 2.0, Sim: 0.5623))\n",
      "---\n",
      "Mean Absolute Error: 0.5000000000000001\n"
     ]
    }
   ],
   "source": [
    "# Combina gli embeddings, dando maggiore peso alla keyword (es. 0.7) rispetto alla descrizione completa (es. 0.3)\n",
    "combined_embeddings = 0.7 * embeddings + 0.3 * embeddings_kw\n",
    "\n",
    "test_indices = np.random.choice(len(df), size=3, replace=False)\n",
    "train_indices = np.setdiff1d(np.arange(len(df)), test_indices)\n",
    "\n",
    "k = 3\n",
    "results = []\n",
    "for test_idx in test_indices:\n",
    "    test_keyword = df.iloc[test_idx][\"ChoreKeyword\"]\n",
    "    true_difficulty = df.iloc[test_idx][\"Difficulty\"]\n",
    "    test_embedding = combined_embeddings[test_idx]\n",
    "    \n",
    "    similarities = [cosine_similarity(test_embedding, combined_embeddings[train_idx]) for train_idx in train_indices]\n",
    "    top_k_local_indices = np.argsort(similarities)[-k:][::-1]\n",
    "    neighbor_indices = train_indices[top_k_local_indices]\n",
    "    \n",
    "    neighbor_details = [\n",
    "        (df.iloc[n][\"ChoreKeyword\"], df.iloc[n][\"Difficulty\"], cosine_similarity(test_embedding, combined_embeddings[n]))\n",
    "        for n in neighbor_indices\n",
    "    ]\n",
    "    predicted_difficulty = np.mean([df.iloc[n][\"Difficulty\"] for n in neighbor_indices])\n",
    "    \n",
    "    results.append((test_keyword, true_difficulty, predicted_difficulty, neighbor_details))\n",
    "\n",
    "mae = np.mean([abs(true_diff - pred_diff) for _, true_diff, pred_diff, _ in results])\n",
    "\n",
    "for test_keyword, true_diff, pred_diff, neighbors in results:\n",
    "    neighbor_str = \"; \".join([f\"{n[0]} (Diff: {n[1]}, Sim: {n[2]:.4f})\" for n in neighbors])\n",
    "    print(f\"ChoreKeyword: {test_keyword} - True Difficulty: {true_diff} - Predicted: {pred_diff:.2f}\\n({neighbor_str})\\n---\")\n",
    "print(\"Mean Absolute Error:\", mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\markh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('italian'))\n",
    "stemmer = SnowballStemmer(\"italian\")\n",
    "cleaning_verbs_stems = {\"pulir\", \"lav\", \"spolver\", \"aspir\", \"sbrin\", \"stir\", \"mop\"}\n",
    "\n",
    "def extract_keyword(chore_str):\n",
    "    tokens = re.findall(r'\\w+', chore_str.lower())\n",
    "    return \" \".join(\n",
    "        stemmer.stem(token)\n",
    "        for token in tokens\n",
    "        if token not in stop_words and token not in cleaning_verbs_stems\n",
    "    )\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / np.sum(e_x)\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "def predict_difficulty(chore_str, k=3, alpha=0.7):\n",
    "    keyword_version = extract_keyword(chore_str)\n",
    "    \n",
    "    full_emb, keyword_emb = model.encode([chore_str, keyword_version], convert_to_numpy=True)\n",
    "    input_emb = alpha * keyword_emb + (1 - alpha) * full_emb\n",
    "    combined_embeddings = alpha * embeddings_kw + (1 - alpha) * embeddings\n",
    "\n",
    "    sims = np.array([cosine_similarity(input_emb, emb) for emb in combined_embeddings])\n",
    "    knn_indices = sims.argsort()[-k:][::-1]\n",
    "    \n",
    "    weights = softmax(sims[knn_indices])\n",
    "    predicted_diff = np.dot(weights, df.iloc[knn_indices][\"Difficulty\"])\n",
    "    \n",
    "    neighbors = [(df.iloc[i][\"Chore\"], df.iloc[i][\"Difficulty\"], sims[i]) for i in knn_indices]\n",
    "    \n",
    "    return predicted_diff, neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted difficulty: 1.6608907282352448\n",
      "Neighbors: [('Pulire il tostapane', 1.0, 0.5945617), ('Pulire il bidet', 2.0, 0.58650476), ('Pulire i tappeti', 2.0, 0.55053353)]\n"
     ]
    }
   ],
   "source": [
    "difficulty, neighbors = predict_difficulty(\"riparare i tubi\", k=3, alpha=0.7)\n",
    "print(\"Predicted difficulty:\", difficulty)\n",
    "print(\"Neighbors:\", neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\markh\\Desktop\\Codici\\litpi\\venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "c:\\Users\\markh\\Desktop\\Codici\\litpi\\venv\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Normalizzazione\n",
    "scaler = StandardScaler()\n",
    "embeddings_scaled = scaler.fit_transform(embeddings)\n",
    "\n",
    "# UMAP migliorato\n",
    "umap_reducer = umap.UMAP(n_components=5, n_neighbors=5, min_dist=0.1, random_state=42)\n",
    "embeddings_umap = umap_reducer.fit_transform(embeddings_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=0.7, min_samples=3)\n",
    "clusters = dbscan.fit_predict(embeddings_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\markh\\Desktop\\Codici\\litpi\\venv\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:151: FutureWarning:\n",
      "\n",
      "'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "\n",
      "c:\\Users\\markh\\Desktop\\Codici\\litpi\\venv\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning:\n",
      "\n",
      "n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "umap_reducer_3d = umap.UMAP(n_components=3, n_neighbors=10, min_dist=0.05, random_state=42)\n",
    "embeddings_3d = umap_reducer_3d.fit_transform(embeddings_scaled)\n",
    "\n",
    "df_3d = pd.DataFrame(embeddings_3d, columns=[\"UMAP_1\", \"UMAP_2\", \"UMAP_3\"])\n",
    "df_3d[\"Cluster\"] = clusters\n",
    "df_3d[\"Chore\"] = df[\"Chore\"]\n",
    "\n",
    "# Grafico 3D interattivo\n",
    "fig = px.scatter_3d(df_3d, x=\"UMAP_1\", y=\"UMAP_2\", z=\"UMAP_3\", color=df_3d[\"Cluster\"].astype(str), hover_data=[\"Chore\"],\n",
    "                     title=\"Clusterizzazione delle Chore in 3D\", opacity=0.8)\n",
    "\n",
    "# Salva e apri il plot in una finestra del browser\n",
    "fig.write_html(\"plot.html\")\n",
    "import webbrowser\n",
    "webbrowser.open(\"plot.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
